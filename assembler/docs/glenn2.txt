I tend to give give long replies.  I've broken this up into a bunch of
separate messages that I'm about to send.

2. Past outlines I made for new assembler development
[Hamid and Pavel already have these]

3. Overview of my EULER 4.0 outline vs. Hamid's EULER 4.0 implementation

4. File formats

5. Quality value scales and file formats
[Hamid and Pavel already have this]

6. Quality values in Illumina reads
[Hamid and Pavel already have this]

7. GraphViz to visualize the assembly graph

8. Sergey's email from last week --- I'll reply later

--------

Assembler_redesign_notes.docx is from July 27, 2010, and listed a lot of
things at a high level.

E4-simplify-graph.pdf is last updated Sep. 20, 2010, and focused on
certain aspects with more details, particularly for items that at the time
we had been in agreement on; Hamid's implementation however has not
followed this path.

For reference, please get the EULER-SR 2.0.0 source code from here:
http://euler-assembler.ucsd.edu/euler-sr.2.0.0.tgz

It does go through all stages of assembly, but it wasn't written to be
parallel and didn't scale well.  The intention is only for you to use it
as a reference.  Only use relatively small inputs on it.  Vs., Hamid's
EULER 4.0 doesn't go through all discussed assembly stages.

--------

EULER 4.0 as Hamid has implemented it, differs from my "EULER 4.0"
outline, so he will have to write a more thorough description of what
he's done.  Here is my brief overview for you of what he's done, and in
some cases, notes on how this differs from EULER-SR or from my outline.

BRIEF OUTLINE:

1) He builds the de Bruijn graph efficiently.

2) He does a limited type of graph simplification.

3) He has a new algorithm for dealing with mate pairs, but it's not
yet done well.


MORE DETAILS (but Hamid really needs to write something):

1) He builds the de Bruijn graph efficiently.

a) It's parallel.

b) Generating the collection of k-mers and their multiplicities:

(i) EULER-SR: Mark did a variation of merge sort, sorting a
portion of the k-mers at a time, building them into larger and larger
lists, merging pairwise when they are of similar size.  E.g.,
Sort & count first x kmers (as we parse the reads sequentially).
Sort & count next x kmers.
Merge them to get ~ 2x kmers (actually less because of duplicates).
Sort & count next x kmers; now we have lists of 2x & x kmers.
Sort & count next x kmers; merge the two lists of size x together.
Now we have two lists of size 2x.  Merge them into a list of size 4x.
...
He does uses trees to do this differently but it's similar to that
description.

Other papers I've read discuss doing parallel sorts to make the list of
all k-mers.

(ii) Hamid's Euler 4.0: He doesn't sort the k-mers.   He stores
them in a "Hash + BST (binary search tree) data structure",
and as well, he distributes this over multiple processors.
I'm working with him on an improvement for how it's distributed
over processors, so that contiguous k-mers will tend to be
allocated to the same processor.
ABySS has elements of this, too.

As the reads are initially read, the Hash+BST structure grows.

After the reads are all parsed, the Hash+BST structure is static, so the
specific location within the BST will stay constant. Thus, a reference to
a kmer, instead of taking 2k bits, can take the # bits needed for hash
index plus BST index plus direction flag (since it's bidirected, and the
same node represents both a kmer and its dual).

b) He uses a bidirected graph.
As explained in my outline, three strategies for dealing with
double strandedness are

(i) doubled graph like EULER 2001,2004,2007: put in the dual
of every read, and explicitly have mirror image components,
and explicitly keep everything in sync.  There's a lot of overhead
in keeping everything in sync, and memory is basically doubled.

(ii) bidirected graph: Velvet does this.  Mark says in retrospect,
he should have done this.  This is what Hamid uses.

(iii) single stranded assembly, assuming the high coverage will
generate both strands, and then reconcile corresponding dual
components at the end.

For the regular de Bruijn graph, I urge you to go with (ii), bidirected
graph.  It's a lot better than trying to keep everything in sync for (i)
as you do graph editing; it will cut down the complexity of doing searches
through the graph, since the graph will literally be half the size of (i)
or (ii); and it's a lot better than trying to reconcile unfortunate
differences in dual components in (iii).

For the approximate paired de Bruijn graph, the construction is
asymmetric with respect to the two reads in a pair, so unless that
can be improved, unfortunately (i) or (iii) is necessary.

That said, I feel there SHOULD be an option for single stranded assembly,
since there are applications where it's what makes sense to do.  But
(iii), single stranded assembly plus reconciling the dual components at
the end, *will* introduce complications, that would be avoided in (i) or
(ii).

c) He squeezes out more memory by overriding compiler word alignment:

* You need ceil(2k/8) bytes to store a {A,C,G,T} k-mer, and this
is increased if you store it in other sized words.
Then the compiler pads it to round it up (e.g.,
to a 64-bit boundary on current machines).

* I think that squeezing out the padding does get us a little more memory
(at most, a factor of 2, but actually less), but reduces the speed.
But the particular way he does it complicates the code.

* EULER-SR uses an array of fixed sized words to store k-mers.
Hamid's EULER 4.0 uses a mix of variable sized words.  There are a
series of conditional compiler directives, so if he needs 6 bytes,
he'll use a 32 bit word and a 16 bit word; if he needs 11 bytes, he'll
use a 64 bit word, a 16 bit word, and an 8 bit word; and then turn off
the normal compiler alignment if possible.  I think this optimization
may go too far, since it complicates the code.

d) Bitwise representation of nucleotides in kmers:

EULER-SR:
Word size and number of words are compile-time options.
Say we are using two 64-bit words to store k-mers.

Nuc. # 0 is stored in word 0, bits 0 & 1
Nuc. # 1 is stored in word 0, bits 2 & 3
...
Nuc. # 31 is stored in word 0, bits 62 & 63
Nuc. # 32 is stored in word 1, bits 0 & 1
...
Nuc. # 63 is stored in word 1, bits 62 & 63
If k<64, truncate that list, and some bits go unused.


Hamid's EULER 4.0:
As described above, he uses variable word sizes.
Suppose we have 22-mers, requiring 6 bytes.  He'll use a 32-bit word and a
16-bit word, and the nucleotides are from high bits to low bits instead,
within each variable sized word.

Word 0 (32 bits):
Nuc. # 0 is in word 0, bits 30 & 31
Nuc. # 1 is in word 0, bits 28 & 29
...
Nuc. # 15 is in word 0, bits 0 & 1

Word 1 (16 bits):
Nuc. # 16 is in word 1, bits 14 & 15
Nuc. # 17 is in word 1, bits 12 & 13
...
Nuc. # 21 is in word 1, bits 4 & 5
Word 1, bits 0-3, would not used for k=22.


e) Compressed sequences:

i) The strategy used in EULER 2001,2004,2007 was to first make
a graph with all vertices = k-mers, all edges = (k+1)-mers.
Then form the branching graph, in which vertices with
(indegree, outdegree) != (1,1)
would remain, while the 1-in/1-out chains of vertices
would be condensed into edges of variable length.
So a chain

 v[0] -> v[1] -> v[2] -> ... -> v[m]

where v[0], v[m] are branching vertices and all the intermediate
vertices have (indegree, outdegree) = (1,1), would represent
a sequence of length k+m nucleotides.  In the branching graph,
vertices v[1],...,v[m-1] are removed, and instead, an edge is
formed from v[0] -> v[m] with that length k+m sequence on the edge.

ii) In EULER 4.0 instead, he defines a structure called a CNode, and he
stores sequences of variable lengths (but >= k) in the CNodes.
Ignoring for a moment that it's bidirected, the chain above would
be condensed into a single sequence of length k+m provided that
  indegree(v[0]) != 1
  indegree(v[i]) = 1 for i=1,2,...,m
  outdegree(v[i]) = 1 for i=0,1,...,m-1
  outdegree(v[m]) != 1
and then this sequence would be stored in the node, not on the edge.
Edges essentially represent (k+1)-mers in this formulation.


2) Graph simplification:
My outline explains things that were done badly in graph simplification
before, and how to redo them.
Hamid hasn't implemented that type of graph simplification yet.
Instead, he implemented a different kind that is aimed at single cell
haploid assembly with uneven coverage.  It's similar to the Velvet-SC
algorithm but w/o Velvet's graph simplification either.  Basically, it's

for t = 1  to  cutoff {
  eliminate all CNodes of average coverage <= t
  merge CNodes that now form a chain w/o branching
}

He had put in code to trim bad read ends, but my understanding is that
he's disabled/removed that.  My feeling is that we should ALWAYS
remove read ends deemed bad by the quality values before ever
buliding the graph, and additional work can be done in the erosion
step of graph simplification.

I feel (as does Pavel) that graph simplification needs to be done.  My
EULER 4.0 outline explains a method of bookkeeping that would be better
than what we did before, in order to keep track of how kmers are remapped
during graph editing, so that there is no incorrect arbitrary guessing
about that later.  It also explains different graph editing operations
we've discussed, and that an operation "edge-only pasting" would probably
be the best one to use.

3) Dealing with mate pairs -- his current algorithm is incomplete.

----------

As to file formats and data structure formats:

FYI, Mark's strategy in writing EULER-SR was to replace one part of EULER
2004 at a time, allowing it to use the old format, or to use his newer
format, until all of EULER 2004 was replaced.

That said, there is no need to maintain the same internal file formats.

INPUT FILES:

* I would allow .fasta and .fastq (there are several versions of quality
value file formats --- I'll send separate email on that too), and the
colorspace versions.

* Each manufacturer has it's own sequence file formats that can be
converted to these.  The EULER-SR source code bundle has converters.

*  David Haussler has a project (with PacBio) to make a new input file
format that replaces nucleotides and quality values, by "machine codes".
Since you have so many people working for you, it's probably worth it to
assign someone to look at the details.

* The mate pair rules files from the original EULER 2001, 2004, and Mark's
from EULER 2007, are for Sanger sequencing characteristics only.  Hamid's
mate pair configuration file is a better approach.  For each library, it
lets you independently set the directions of both reads.  It puts bounds
on the spacing.  And it can be extended to strobes when PacBio releases
more details.

Additionally, EULER 2001,2004,2007 base mate pairs on some pattern in the
FASTA read names, e.g., foo.x and foo.y, where "foo" is unique to that
pair, "." is some delimeter, and "x"/"y" are the two in the pair.  It
would be much better either to have them in the same file consecutively,
or to have them in separate files that are coordinated (pair x is read # x
in both files).  We'll have to see what PacBio's format is.

INTERMEDIATE FILES:

There is no need at all to be compatible with these.

FYI, EULER-SR generates files for four phases of assembly:
"fixed" -- initial graph (after error correction)

"simple" -- after graph simplification

"transformed" -- after equivalent transformation by the paths defined by
individual reads

"matetransformed" -- after equivalent transformation by the paths defined
by paired reads

The files within these various stages have tables of k-mers and their
counts; graph topology; sequences on the edges; how each read maps to each
contig; and GraphViz format files to represent pictures of the graphs.
My writeup goes into how the .intv file representing how reads map
to contigs, could be improved.


OUTPUT FILES:

I would recommend this:

* A FASTA file with the contigs.

* A .sam file (or a file that can be converted to .sam format) showing
exactly how every original read in a contig was mapped to the contig,
according to the assembler (not according to Bowtie's guess).  Of course
you've gone through read trimming, error correction, graph simplification,
and whatever other steps, but this should show the alignments of the
original reads before all of that to the final contigs after all of that.
It should be based on careful bookkeeping by the assembler of how it
manipulated each read, not on trying to align reads to contigs later,
since whatever alignment program you use will not accurately represent how
the assembler manipulated the read.

* GraphViz files (as an option).  These show the vertices & edges of the
branching graph (or the CNodes & edges in Hamid's EULER 4.0).  These files
are pretty unwieldy and it would be great to use an alternative (if one
exists) or to develop one; however, I don't know a good alternative yet.
GraphViz files are easy to output, so even if there may be some
alternative in the future, it should still be implemented now.  The
purposes of GraphViz files include visualizing:

a) The repeat structure of the graph.  This assumes the graph has been
very simplified so all the bulges and other small features are gone.  This
graph would be small.

b) The messy structures (bulge networks, etc.) in the graph that need to
be simplified, or which are standing in the way of determining unique mate
pair paths, etc.  This graph could be unwieldy.


In EULER 2001,2004,2007, coverage was low enough that GraphViz files
weren't too unwieldly.

With current sequencing files, they are quite large.  I'll write a
separate email about how I recommend to visualize it within the
constraints of GraphViz.

-----------

Here is a paper from last year that gave an overview of quality value formats:
http://nar.oxfordjournals.org/cgi/content/full/gkp1137

and a Wikipedia article with a lot less detail:
http://en.wikipedia.org/wiki/FASTQ_format

Most quality value formats are based on the PHRED scores and the issue is that
they encode the scores differently; but once decoded to a number, they mean the
same thing.  However, there is a second scale in use, introduced by Solexa, and
present in earlier Illumina data since Illumina bought Solexa. The Solexa scale
is also used in MAQ.


For Phred scores:

When they calibrate the sequencing machine, they group together all calls that
have similar conditions in interpreting the signal from the sequencer.  If
those conditions lead to a probability of error P_e (between 0 and 1), they
report a quality value
   q = -10 * log10(P_e)

(that's a numeric value >= 0; of course the fastq file encodes that in a
special way, which will be described later).

That means that given a quality value q, the probability the base is in error
is
    P_e = 10^(-q/10)

and the probability it is correct is
    1 - P_e = 1 - 10^(-q/10).

This leads to:

Given string s with bases b[1], b[2], ..., b[n]
and quality values q[1], q[2], ..., q[n]
the probability it is correct is
    = product_{i=1..n} (1 - 10^(-q[i]/10))
Use that to initialize p0(omega) in place of your current eqs (1)-(2)


For Solexa/MAQ scores: instead of q=-10*log10(P_e), they define
q_Solexa = -10 * log10(P_e / (1 - P_e))
so the probability of error is
   P_e = 10^(-q/10) / (1 + 10^(-q/10))
the probability of a correct call is
   1 - P_e = 1 / (1 + 10^(-q/10))
and the probability a string as above is correct is
   = 1 / product_{i = 1..n} (1 + 10^(-q[i]/10))


Encodings of quality values in the .qual / .fastq files: In the discussion
above, q could have been a floating point number, but they actually discretize
it in all of the encodings, by rounding it off to the nearest integer before
encoding, and then they encode it as follows:

1) Integers written out in ASCII format (separate .fasta and .qual files):
Around since Sanger reads.  Currently used by 454, and a variant is used by ABI
for colorspace.

Here's an ancient sample file from the website for the original EULER:

reads:
http://nbcr.sdsc.edu/euler/demos/M_BA0093H02/M_BA0093H02.RAW.fasta.screen

quality values:
http://nbcr.sdsc.edu/euler/demos/M_BA0093H02/M_BA0093H02.RAW.fasta.screen.qual

It's encoded as ASCII numbers, e.g.:
A read in the .fasta file:
> > abv77a02.s1  CHROMAT_FILE: abv77a02.s1 PHD_FILE: abv77a02.s1.phd.1 CHEM: prim
> > DYE: ET TIME: Wed May  3 08:06:07 2000 TEMPLATE: abv77a02 DIRECTION: fwd
GATGTGTGTXXXXXXXXXXXXXXXXXXXXXXTTCACTAAGACATTTAAGC
AATGGCTGGTGAGATGGCTCTGTGAGTAGGGTTTTTGCCATGCAAACATG
AGGTTCTTAATTCATATCACCAACACCACGTAGGAAGACGGACATGGTAG
TACTTACTGTAGGGCCAGCACTGTGGGGTAGACAAAGGTCTTGCTGACCA
...

And its quality values in the .qual file:
> > abv77a02.s1 PHD_FILE: abv77a02.s1.phd.1
7 9 10 10 10 10 10 16 10 10 11 22 19 19 17 17 10 10 15 20
27 38 38 39 47 39 39 39 39 47 47 44 44 44 39 39 44 47 47 39
39 39 44 44 47 47 59 39 39 39 51 51 51 62 62 62 59 59 59 59
59 59 59 62 62 59 59 59 59 59 59 59 59 59 62 62 59 59 59 59
...

Note base calls and quality values are in separate files but with the
reads in the exact same order, and the read name (before the first space on the
">" line) must match but you can put other info after it.

Note it is ALSO possible to have these integer values in the .fastq file
rather than a separate qual file.  Bowtie calls this "--integer-quals".

Note that ABI colorspace format still uses a separate .qual file in the exact
format that I gave above, but the read file format is different (.csfasta) due
to colorspace being different from basespace.

2) Single ASCII characters (.fastq files with reads and quality values
combined):

See Table 1 in the NAR paper linked to above for some of these; to get the 'q'
in the above formulas, you take the ASCII character code (some subinterval of
32 .. 127) and subtract the offset listed below.

NAR article "fastq-sanger"
Bowtie option "--phred33-quals"
uses PHRED scale.
ASCII offset = 33
ASCII range 33 .. 126
Phred range = 0 .. 93

NAR article "fastq-solexa" (Solexa & early Illumina):
Bowtie option "--solexa-quals"
Uses Solexa scale.
ASCII offset = 64
ASCII range 59 .. 126
Solexa range = -5 .. 62

NAR article "fastq-illumina" (we should call it "fastq-illumina1.3"):
Bowtie option "--solexa1.3-quals" or "--phred64-quals" (equivalent)
I think we should allow "--illumina1.3-quals" too.
Uses PHRED scale.
ASCII offset = 64
ASCII range 64 .. 126
PHRED range 0 .. 62
As you know, Illumina assigned a special meaning to ASCII 'B' (PHRED score =
2).  That does not apply to quality scores from other vendors,
and the definition of the 'B' call has changed in different versions of the
Illumina pipeline.

Not in the NAR paper:
The Illumina 1.5 pipeline changed the use of quality values 0, 1, 2 (ASCII
characters '@', 'A', 'B').
On the PHRED scale, they would mean
     10^(-0/10)=1, 10^(-1/10)=.79, 10^(-2/10)=.63
They don't use quality values 0 or 1 any more.
They changed 2 to be allowed anywhere in the read, not just
at the end of the read.

Perhaps the best way to deal with this is to split phred64-quals as follows:

--phred64-quals: no special meaning for 'B'.  So use it for non-Illumina
platforms that have this encoding.

--solexa1.3-quals in Bowtie / --illumina1.3-quals
(and potentially --illumina1.5-quals):
like --phred64-quals but with a special meaning for 'B'.
Or maybe it doesn't matter; in general, they only use 'B' when they consider it
to be crap.  It's true that you found it may be correct anyway, but really
there is no problem with being conservative.


Please implement all of the formats natively.  We will have to confirm with
Illumina which version they used for the earlier data they sent us (including
"E. coli normal") -- the original Solexa scale or the Illumina 1.3 scale.

-----------

At ISMB today, I spoke with some people about quality values in
Illumina sequencing: Ole; Klaus Maisinger from Illumina UK, who
implements quality value calling for Illumina; and Martin Kircher from
Max Plank Institute, who is working with Illumina on improvements to
base calling and quality value calling.

Of course this is not the main focus of the conference, but it is a topic that
has been relevant for us so I am solely focusing on it in this email.

Here are some details, and in particular, I mark two items [*****]
that would be good for you to implement.

1. On the Phred scale, quality value Q means that the probability of an error
at that position is

10^(-Q/10)

The way that they calibrate it is that they take a known reference genome
without variation (PhiX is good; a standardized bacterial line could work;
human is problematic because the genome is diploid and there will be
heterozygous sites, but they can even handle that by masking out all SNPs
cataloged in dbSNP).

They generate reads, and in addition to the base sequence, they also use the
original image data for that read.

They map the reads to the genome and declare each base either to
be a match or a mismatch.

For each base of each read, they look at factors in the original
image: the intensity profile used for calling that base; whether it is
in a homopolymer; and other factors.

For that profile (intensity, etc.) they form counts over all bases
in all reads that have that profile:

A = # correct calls with that profile
B = # incorrect calls with that profile

Then they determine the error rate for that profile
B / (A+B) = (# incorrect calls) / (# correct calls + # incorrect calls)
and transform it to the Phred scale
Q = -10 * log10(B / (A+B))

They say that they have calibrated it and tested it and the quality
values are very accurate.

2. Regarding the two plots you did of quality values:

For matches, you plot a histogram with x-axis = quality values,
y-axis = # match positions with that quality value
(variable A above)

For mismatches, you plot a 2nd histogram with x-axis = quality values,
y-axis = # mismatch positions with that quality value
(variable B above)

Those aren't conditioned in the way that they calibrate them.

[*****] Please try the following plot instead (called a QQ plot):

For each quality value Q, let
A = # matches (over all bases in all reads) that have quality value Q
B = # mismatches (over all bases in all reads) that have quality value Q
Q' = -10*log10( B / (A+B) )
plot the point (x,y) = (Q, Q')

The QQ plot should come out to be pretty close to the line y=x,
with the exception of Q=2, which they redefined specially.
For other platforms too, the QQ plot should be close to the line y=x.

[I'm aware that Illumina has used scales that are not quite the Phred
scale, but those have alternative formulas that can be used instead.]

3. Regarding the code Q=2 (symbol "B" in quality value strings):
As previously discussed, they do not use Q=2 by the Phred scale definition.
Instead, they have filters to flag the base/read when the profile
(intensity, etc.) is deemed suspicious, in which case they may flag
the base by using quality value = 2.

I asked them about your observation that bases coded this way (and reads with
lots of such quality values) often map correctly.
They say that they know this: their filters err on the side of
calling more things bad than actually are.

The reason they chose to use Q=2 is that this is a quality value that does not
actually arise.  On the Phred scale, Q=2 means the probability the base was
called incorrectly is

10^(-2/10) = 63%

and if it was that bad, they would not be reporting that base anyway,
but would be reporting a base "N".


4. [*****] Using quality values, the probability a read or substring
of it is correct is

substring:      x_1 x_2 ... x_k
quality values: q_1 q_2 ... q_k
P(x_i incorrect) = 10^(-q_i/10)
P(x_i correct) = 1 - 10^(-q_i/10)
P(substring correct) = product_{i=1..k} ( 1 - 10^(-q_i/10) )

That would be easy to incorporate into the probability products you
are already doing.

5. Trimming ends of reads was mentioned in a number of talks and
posters in the high throughput sequencing session.  EULER is not
unique in this regard.

-----------

GraphViz files:

I recommend to make GraphViz files that can be output at intermediate
stages of the assembly (for debugging) and at the end (to show the repeat
graph).

However, since the read files have grown to be so large, and more
erroneous reads lead to larger graphs, the graphs are so big now that
GraphViz doesn't easily render it.  So I recommend to split the graph up
into a lot of smaller files, as described below.

* If you make the graph like EULER 2001, 2004, 2007 do, with contigs on
edges:

vertices = branching vertices, sources, and sinks of the graph,
representing k-mers

edges = 1-in/1-out chains, representing contigs and their duals.  Sequence
lengths are much bigger than k

* If you make the graph like EULER 4.0 does, with contigs in vertices
(CNodes):

vertices = CNodes, representing long sequences

edges = (k+1)-mers, no need to show them since they are implied by the two
vertices they connect.

* labels on vertices and edges: The actual sequences may be too big to
display on the graph.  But put on node or edge ID numbers, and a few
statistics such as number of read intervals in an edge/CNode; average
coverage within the sequence (sum of # nucleotides in the read intervals,
divided by number of nucleotides in the edge/CNode sequence)

* To deal with how large the graph is, break it up as follows:

1) Simple components with one edge only can all be dumped into one file.

2) EULER-SR puts other components to separate files.

3) But I would further break up the components if it's before
simplification, since they are just too large.  Allow the user to set a
threshold.  Split each edge or CNode sequence that is above this
threshold.  This breaks the components into a bunch of smaller components.
Use special GraphViz node shapes / edge arrowheads to show when a node or
edge has been broken in this manner.

The idea is, if you are eliminating bulges where each path must have size
< x to be considered, then you can set a threshold a little bigger than
x (x+2k may technically be enough but you probably want it bigger).  Bulge
removal cannot possibly deal with any features that are longer than this
threshold, so this reduces each "broken" component down to the just the
subgraph that simplification can possibly address.

Visualizing the graph may be helpful in redesigning graph simplification.
Visualizing the coverage info on the graph may be helpful in determining
how to distinguish errors vs. variations from repeats vs. variations from
diploid genomes vs. ...

Similarly, if you are doing mate pair transformations after graph
simplification, and you have libraries with insert size y, then set the
threshold to be a bit bigger than y; e.g., maybe 2*y.  Any contig already
longer than y cannot be spanned by a mate pair, so such contigs are a good
place to break the graph at.  If you manage to chain multiple mate pairs
together to effectively get longer mate pair spans, adjust the threshold
accordingly.

Glenn
