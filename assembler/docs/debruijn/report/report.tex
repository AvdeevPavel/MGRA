\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cmap}
\newcommand{\remark}[1]{\textbf{Remark:} #1}
\newcommand{\dbg}{de Bruijn graph}
\newcommand{\ignore}[1]{}

\title{De Bruijn project}
\author{Sergey Nurk, Anton Bankevich and Nikolay Vyahhi}
\date{\today}	

\begin{document}

\maketitle

\tableofcontents

\section{De Bruijn graph}

This section describes our approach to the condensed \dbg{} representation, construction and modification.

Condensed graph architecture was designed to be convenient, flexible and easily extensible. 

It has reach interface that in particular provides different kinds of modifying operations. 

One can create additional information storages and automatically synchronize them with the graph's state without a need to change a graph core or processing algorithms!

%structure such that without changing core of this structure one could easily use this graph, change it, create additional information storages and synchronize information while graph changes. Space and time efficiency did not have the first priority but where possible (or necessary) these parameters were also taken in consideration.

\subsection{Definitions}

Conjugate --- synonym for reverse-complement.

\subsection{Condensed graph representation}

We work with condensed graph \dbg{} with $k$-mers on vertices and nucleotide sequences of arbitrary length $\geq k + 1$ on its edges. Sequence of first $k$ nucleotides of any edge is equal to it's staring vertex's sequence. Sequence of last $k$ nucleotides - to it's ending vertex's sequence. It is assumed that $k$ is odd so that no vertex can appear to be self-conjugat.

Our graph explicitly supports conjugate structure. It means in particular that by adding an vertex/edge to the graph we instantly add its conjugated. After that we provide an easy way to get conjugated element for any graph element.

% Usually in addition to the condensed \dbg{} structure we will need index on its $k+1$-mers ($k+1$-mer index). It is a mapping from each $k+1$-mer contained in the graph to a pair of an \emph{edge id} it belongs to and its \emph{offset from the start} of that edge. (see 1.5 or what?)

Each of graph edges contains full sequence of nucleotides corresponding to this edge. Two edges conjugated to each other share the same underlying sequence for memory saving.

\subsection{Condensed graph construction}
The construction process consists of two phases:
\begin{itemize}
\item original (uncondensed) \dbg{} construction step
\item condensation step
\end{itemize}

We use implementation of original \dbg{} over the set of $k+1$-mers. 

After some experiments we started using cuckoo hashset (\url{http://en.wikipedia.org/wiki/Cuckoo_hashing}) as an underlying set implementation.

During condensation step we construct condensed \dbg{} and $k$-mer index at the same time.

Condensation is done by the traversal over the \dbg{} edges and accumulating edge-sequence while we are on a simple path (without branches). Obtained sequences are added to the condensed graph as edges and are immediately indexed.

%The result is stored in \texttt{DeBruijnGraph} class which is much more complex structure than SeqMap, used to store initial graph. Since condensed graph has smaller size time performance of graph operation becomes less critical thus certain operations are implemented less time-efficient while more usable.

\subsubsection{Problems}
There are only minor problems in this graph construction concerning time and space efficiency which are not of much significance while applying these code to bacterial genomes. %Even if these problems become bottleneck we could always switch to A-bruijn style graph construction.

%\subsubsection{Suggestions}

\subsection{Condensed graph interface and modification}

Our condensed graph has restricted number of ways to change its contents like add vertex or split edge etc. Each of them is represented by certain public method of \dbg{} class.

All modifying operation can be nominally divided into low- and high-level operations.

Low-level operations are addition/deletion of edge/vertex.

High-level operations are:
\begin{itemize}
\item \textbf{Concatenation of a path}. Path is sequence of edges. Two adjacent edges can be concatenated only if their common vertex is 1-in/1-out.

\item \textbf{Split of an edge in a given position}. Split of an edge $e$ with parameter $pos$ creates two new adjacent edges $e_1, e_2$ s.t. $length(e_1) = pos$ and $length(e_2) = length(e) - pos$ along with their common vertex. % is it true?

\item \textbf{Projection of one edge to another}. Projection of edge $e_1$ to $e_2$ in terms of structural changes results in deletion of $e_1$ and $e_2$ and creation of new edge $e$ with the same sequence as $e_2$ and with merged assosiated information (from $e_1$ and $e_2$). We need this operation to move all associated information from edge $e_1$ to $e_2$. 

\end{itemize}

% Edges are concatenated immediately with each opportunity.

During graph modification it is important not to break consistency of conjugate elements. 
To achieve this every time a vertex/edge is added/deleted from the graph, conjugate element is added/deleted as well.
Since high-level operations are implemented with low-level operations, this strategy appears to be sufficient for high-level operations to work correctly out of the box in most cases. \footnote{The only special case is concatenation of the path that contains self-conjugate edge and of course edge cannot be projected to its conjugate}

%To achieve this every time a modifying operation is performed, the symmetrical operation is performed on conjugate elements. Implementation of this approach appeared to be tricky at some moments, but after it was done it allowed us to think of conjugateness much more seldom. 

What makes our condensed graph convenient and extendible is using of callback technique. Each time a modifying operation is performed corresponding event is triggered and all objects that listen to graph events are notified about what exactly happened to the graph.

There are two moments that were taken into consideration during implementation of this concept:
\begin{itemize}
\item we should immediately call handlers for conjugate operations too
\item order of handler calls during high-level operations is not obvious from the first sight
\end{itemize}

This concept allows to implement any additional graph information as an external structure and hide its maintaining logic from the graph and from processing algorithms. 

\subsubsection{Problems}
\begin{itemize}
\item Code is argued to be over-architectured. In particular all GraphHandler calls are virtual and it is not yet clear if there is performance bottleneck here.
\item Graph still has different core from A-bruijn graph thus major refactoring is needed. It will be finished in the near future (should implements AbstractConjugateGraph),
\item Initially graph was meant to be used for error correction thus operations necessary for repeat resolving are still not supported and even interface needs further discussion. Also certain problems arise concerning support of additional information which would be discussed later (coverage / paired info). Also, there will be problem with index while unglueing, since it's not multimap but only map from $k+1$-mers to edges.
\end{itemize}

\subsection{Coverage}

Coverage of an edge here corresponds to the number of times ($k+1$)-mers from this edge were observed in original reads. It's additive, so when we concatenate edges coverage of result is equal to sum of coverages of edges concatenated. At the moment coverage is stored in edges of graph and maintained with Handler.

\subsubsection{Problems}

Coverage structure itself seems to have no problems but it is not clear how to support coverage values in unglueing operations used in repeat resolving. When one edge is unglued to several new edges one can not tell which part of edge coverage supports which edge. Actually this problem could be just ignored because when certain ungluing operation is performed one can rely on all the new edges to be trusted thus actual value of coverage does not really matter while we do not try to estimate edge multiplicity by its coverage.

% \subsubsection{Suggestions}

\subsection{Index}

In order to be able to map reads to graph index structure is maintained. Index contains map from k+1-mers to positions at edges where these k+1-mers are in graph.

\subsubsection{Problems}

It is not clear if this whole heavy structure is actually necessary. Maybe only neighbourhoods of vertices should be indexed (with respect to gap size).


\subsubsection{Suggestions}
Memory needed can be decreased to half if storing info only for lexicographically min of conjugate $k+1$-mers

\subsection{Unique ids}

Currently id of vertex or edge is an address of corresponding object in memory. As the result ids are unique at any given moment of time but if one considers all vertices or edges which were in graph while program worked ids become not unique because memory where vertex/edge object was stored at some point could be reused later and thus it would have the same id. In order to deal with this problem UniqueIdStorage structure was suggested (not yet implemented). This structure allows to assign each vertex and edge globally unique id and then find vertex with given unique id or unique if of the given vertex.

\subsection{History}

\subsection{Paired Info}

Paired info structure stores information about paired edges. It's actually map $$(edge, edge, distance) \Rightarrow weight$$ 
For each two edges one can request list of all $(distance, weight)$ pairs for this edges. Also it's possible to request information about just one edge (i.e. to which edges it's connected and corresponding distances and weights).

%about clustering
Pair info storing itself is quite easy task while deciding on the form paired info should be stored in is much more sophisticated. Insert length has large dispersion value thus clustering of some sort is required. There are two types of clustering each of which are implemented: online clustering and offline clustering.

There are several points concerning paired information that should be taken into consideration:

\begin{itemize}
\item The earlier reads are mapped to graph the easier this procedure is. If reads are mapped to graph before tip clipping they map to graph perfectly. If reads are mapped to graph after tip clipping but before bulge removal ends of certain reads could have been removed from graph, but mapping is still straightforward. If reads are mapped to graph after bulge removal read mapping becomes complicated task (and is actually called read threading).

\item The earlier reads are mapped to graph the larger amount of space is required to store this information. This is natural since number of edges significantly drops after graph simplification. And also edges become longer and thus lots of paired reads have both reads from pair mapped to the same edge,

\item The later offline clustering is performed the more precise result is. Longer edges contain more info. Thus average values are closer to real distances (than with earlier offline clustering).
\end{itemize}

Currently the following clustering strategy is implemented: first even before tip clipping and bulge removal data is collected from paired read and as the result for each pair of edges we have collection of possible distances paired reads vote for. Then this information is carefully clustered and result of this clusterization is the one actually used to resolve repeats, construct rectangle graph etc.

\subsubsection{Online Clustering}

While changing the graph one should synchronize paired info with these changes. Thus there should be methods to change and merge paired information.

\subsubsection{Offline Clustering}

Input data for offline clustering is collection of paired data with high variance value and output is meant to be carefully clustered information which aims to be as precise as possible.

\subsubsection{Weights}
In order to measure level of certainty paired reads give for this distance each information unit has weight value. Currently there are three ways to count weights being argued.

\begin{itemize}
\item \textbf{Any $k+1$-mer from one read with any $k+1$-mer from another read -- quadratic}. Currently implemented this way, i.e. weight is number of pairs of $k+1$-mers from paired reads that support this distance. For example if first read of pair intersects with sequence stored in edge by $a$ nucleotides and second read of pair intersects with sequence stored in the other edge by $b$ nucleotides than information contributed by this read pair for this pair of edges is $(a - k)(b - k)$. Advantage of this way to measure weight is that it can be easily updated since it is additive with respect to \textit{concatenate} and \textit{project} operations.
\item  \textbf{$i$-th $k+1$-mer from one read with $i$-th $k+1$-mer from another read -- linear}. I.e. weight is number of pairs of $k+1$-mers from the identical positions of paired reads that support this distance.
\item Weight of paired information is a number of reads that support this information (each mate pair gives only $+1$ to distance). This weight is very difficult to support but it may be better at reflecting level of certainty in this information.
\end{itemize}

\subsubsection{Problems}

The main problem here is how to distribute paired information among new edges when unglueing operations are performed on graph. This problem can not be ignored since we can neither copy information to all new edges because we then have lots of mistakes in paired info nor can we forget about this because lots of paired info would be wasted.

\subsubsection{Suggestions}

The only solution we thought of is to make paired info to support a job for repeat resolver itself. This solution breaks concept that information storages should be synchronized with graph by listening events.

\section{Graph tools}
\subsubsection{Smart Iterators}

Smart iterators are tools to iterate through vertices/edges of graph. These iterators were called smart because of two reasons:
\begin{itemize}
\item Smart Iterators is not invalidated after insertions/deletions of edges/vertices to the graph. Moreover Smart Iterators gaurantee that when iteration is finished all vertices/edges which currently belong to graph were iterated. This became possible because of Handler structure. Smart Iterators themselves are handlers and listen to add/delete edge/vertex events.
\item Smart Iterators allow to choose order in which vertices/edges are iterated. This is done with comparator that defines order on vertices/edges.
\end{itemize}

\section{Simplification algorithms}
Simplification algorithms that we are currently applying are:
\begin{itemize}
\item Tip Clipping
\item Bulge Removal
\item Removal of low-coverage edges
\end{itemize}

\subsection{Tip Clipping}

The following algorithm is used for tip clipping: we iterate through all edges of graph in order of increasing length. If current edge topologically looks like tip and has alternative path with good enough coverage this edge is removed. If after removing this edge, vertex that was adjacent with removed edge has unique incoming edge and unique outgoing edge those edges are concatenated and new edge is also checked as tip candidate.

In more precise words edge $e$ judged as tip if:
\begin{itemize}
\item End of the $e$ has no outgoing edges.
\item $length(e) \le max_{len}$
\item $coverage(e) \le max_{cov}$
\item Start vertex of $e$ fas at least one outgoing edge except $e$.
\item $coverage(e) <= max_{relcov} * max_{compcov}$ where $max_{compcov}$ is maximal coverage over all outgoing edges of start vertex of $e$.
\end{itemize}

\subsubsection{Problems}
One of the most common type of error that is not currently corrected by error correction procedures presented is a situation with two tips having there ends glued. Common end of two tips is not removed since there is no alternative path for this edge. And even if this tip is deleted two tips with common end vertex can not be removed by the procedures described. Thus TipClipping procedure needs furether adjustment.

\subsection{Bulge Removal}
\textit{Simple bulge} is a rather short edge $e$ with an alternative path $P$ between its start and end of length $\approx length(e)$.

Our current approach is iterative removal of simple bulges. This strategy allows to delete pretty complex bulge structures and appeared to work pretty well if we alternate it with low coverage edge removal.

Search of simple bulges is pretty straightforward.
We iterate through all edges with smart iterator in order of increasing length (as in tip clipping). For every edge we look for alternative paths from its start to its end within a bounded length neighbourhood.
Among all such paths we choose the most covered one (compared by weighted average coverage of edges in the path).  

If the path $P=\{e_i\}_{i=0}^{p}$ was found then we should make a decision if edge $e$ should be removed from the graph. If we decided to delete $e$ (conditions will be explained below), then:
\begin{itemize}
\item split $e$ into $p$ edges $\{e'_i\}_{i=0}^{p}$ in the same proportions as $\{e_i\}_{i=0}^{p}$
\item project every $e'_i$ onto $e_i$
\item try to condense edges in the neighgourhood of start/end of $e$ (new edges are implicitly included into traversal by smart iterator)
\end{itemize}

As for now, we do NOT delete edge $e$ if any of these holds:
\begin{itemize}
\item $coverage(e)>max_{cov}$
\item $coverage(e)>max_{gap}*coverage(P)$
\end{itemize}

Maximal bulge length, allowed variation of alternative path length, $max_{cov}$ and $max_{gap}$ are all user-defined, but some of them have reasonable default values.

\subsection{Removal of low-coverage edges}

We remove edges with low coverage and small length (as erroneous).

\end{document}