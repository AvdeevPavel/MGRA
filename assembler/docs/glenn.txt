Regarding graph simplification procedures that Pavel has described, to simplify the graph before trying to resolve mate pair paths and repeats:

In the 2004 and 2007 versions of EULER, the graph simplification procedures introduced artifacts in the graph, and there was insufficient bookkeeping to determine exactly where collapsed features were remapped.

In the document "E4-simplify-graph.pdf" that we've forwarded to Max (and I assume he forwarded to you), I summarize discussions Pavel, Hamid, and I had about improvements to graph simplification, to avoid creating particular artifacts, and a method to do bookkeeping to keep track of how reads are remapped.  Here's some more background on that.

1. EULER 2004: The graph was directed, but we looked for long paths
through bulge networks by forming an MST, ignoring edge directions,
and then we added some edges back in.

Ignoring edge directions resulted in "zig zag paths" and in "whirls"
that did not represent true variations in the sequence, but were just
artifacts of how it was simplified.  "zig zag paths" are only artifacts, never something real; "whirls" can be artifacts, or can be real.

The reads were Sanger reads, which were very long (variable length, ~500-1000 nt) compared with k (fixed, ~25), so it was usually (but not always) possible to locate where a read should be remapped, by examining its surviving k-mers.

2. EULER-SR: Initially, Mark Chaisson did a similar algorithm, but using a spanning arborescence.  However, he eventually changed it completely.  He looks for pairs of short directed paths emanating from the same vertex v and both landing on the same vertex (say w), then glues the first condensed edge of them together.  Several artifacts may be introduced by this, including:

* Even if the paths have similar length, the first condensed edge on each of the two paths have different lengths, so gluing them together creates the appearance of an indel. He was dealing with 454 reads that had high indel rates, so he did not consider gluing things of different lengths to be a problem.  It would have correctly glued indels together that formed simple bulges (no branching vertices between v and w) but would have created artifacts otherwise.

* The two paths may be broken into a different number of condensed
edges, so after pasting one condensed edge to one condensed edge
over and over again, there is a remainder that forms a whirl, even if the two paths start with the same number of nucleotides.  So he avoided the type of whirls that EULER 2004 created as artifacts, but introduced whirls as an artifact by his different procedure. :)

* The act of pasting edge segments together, and their vertices, introduces additional connectivity in the graph that wasn't initially present.  It also can result in vertices of very high degree.

He does do some bookkeeping to keep track of where reads are remapped, but it was imperfect.  Separately, a student here tried using Bowtie to determine where reads are mapped to.  But it is optimizing something completely different from how EULER simplifies the graph, so it was unable to map a lot of the reads.  So it's necessary both to improve the graph simplification, and also to improve the assembler's bookkeeping to know and report how it remapped reads.

3. The "EULER 4.0" outline of mine "E4-simplify-graph.pdf" (as opposed to Hamid's EULER 4.0 implementation): Pavel, Hamid, and I discussed how to eliminate the artifacts above, different graph simplification operations, and bookkeeping issues.  Note that it's in my "EULER 4.0" outline, and Pavel and I consider something like it to be necessary, but this is not what's in Hamid's "EULER 4.0" code.

Graph simplification operations:

* We considered a number of operations, and decided that a specific combination of two ("edge-only pasting" and deleting edges) was best in terms of cleaning up the graph w/o creating artifacts, and being able to do bookkeeping.

The outline describes a union-find structure "subs" saying how edges are either deleted, or remapped to do edge-only pasting.  Since we already have to create a collection of all (k+1)-mers (we use k-mer for vertices, (k+1)-mer for edges), if it's feasible to create the collection of (k+1)-mers then it's feasible to create the union-find structure. Similar bookkeeping based on how condensed edges remap potentially would take take less memory, but the bookkeeping would probably be more complex, because the condensed edges keep changing lengths during simplification, vs. the table of (k+1)-mers is fixed once and for all after the initial graph.

* Bulge networks -- the "edge-only pasting" operation remaps edges,
not vertices, so it avoids rerouting other paths that went through
those vertices, and it avoids increasing vertex degrees.  In the
outline, we describe how to prioritize certain bulge configurations
to simplify.

* Zigzag paths -- not an issue; that was solely an artifact of
a particular simplification algorithm.

* Whirls -- these can be produced as artifacts of graph simplification,
but they can also be real.

* Erosion and bad read ends -- bad reads should be rejected based on quality values and flags in the initial input files.  Bad read ends (giving a source or sink in the graph) should be trimmed based on quality values, and may still give stray read tips in the graph that should be removed by erosion.  But instead of using a constant number of erosion cycles, we should look at the length of the particular read involved, and make sure the read tip is from one read, not from a chain of them.  Also, sources/sinks in principle could be due to, e.g., tandem inverted repeats. Hamid implemented some version of this for his EULER 4.0.

* "Thorns" -- not described in the outline.  See Son Pham & Pavel's DRIMM-Synteny paper.  This is a feature in A-Bruijn graphs for rearrangements between multiple species, using an alphabet of gene numbers rather than an ACGT alphabet.

* I sent Max an email, which I understand he has or will share with you, regarding visualizing regions of the graph with GraphViz, and how to deal with graph size limitations in it.  I suggest generating pictures of local regions in the graph with messy features, to assist in working out how to simplify them w/o creating new problems.  In the draft, I describe prioritizing bulges to collapse based on the complexity of their topology. With actual pictures of the messy graph in local regions, instead of abstract pictures, you could get more examples of what it can't simplify to better work out the minutae.

* Your probabilistic formulation potentially could be used to select the right paths through a bulge network.  But I suggest to still consider the bookkeeping issues and valid operations above, and the GraphViz pictures. What might be more practical than optimizing on a global scale is to just pick somewhat arbitrary paths through each bulge network; do transformations by long reads, mate pairs, strobes, ..., and perhaps at the same time, separate some reads based on that; and then afterwards, do a consensus step.  The consensus step could be a multialignment, but it also could be based on rebuilding a local region of the graph (hopefully with a reduced number of reads after separating some repeats), and applying your probabilistic formulation to such smaller graphs.
