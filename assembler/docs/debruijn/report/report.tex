\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cmap}
\newcommand{\remark}[1]{\textbf{Remark:} #1}
\newcommand{\dbg}{de Bruijn graph}
\newcommand{\ignore}[1]{}

\title{De Bruijn project}
\author{Sergey Nurk, Anton Bankevich and Nikolay Vyahhi}
\date{\today}	

\begin{document}

\maketitle

\tableofcontents

\section{De Bruijn graph}

This section describes our approach to the condensed \dbg{} representation, construction and modification.

Condensed graph architecture was designed to be convenient, flexible and easily extensible. 

It has reach interface that in particular provides different kinds of modifying operations. 

One can create additional information storages and automatically synchronize them with the graph's state without a need to change a graph core or processing algorithms!

%structure such that without changing core of this structure one could easily use this graph, change it, create additional information storages and synchronize information while graph changes. Space and time efficiency did not have the first priority but where possible (or necessary) these parameters were also taken in consideration.

It is assumed that $k$ is always chosen even so that no vertex can appear to be self-conjugate.

\subsection{Condensed graph representation}

We work with condensed graph \dbg{} with $(k-1)$-mers on vertices and nucleotide sequences of arbitrary length $\geq k$ on its edges.

Our graph explicitly supports conjugate structure. It means in particular that by adding an element (vertex or edge) to the graph we instantly add its conjugate. After that we provide an easy way to acquire conjugate to some graph element.

Usually in addition to the condensed \dbg{} structure we will need index on its $k$-mers ($k$-mer index). It is a mapping from each $k$-mer contained in the graph to a pair of an \emph{edge id} it belongs to and its \emph{offset from the start} of that edge. 

Each of graph edges contains full sequence of nucleotides corresponding to this edge. Actually conjugate edges contain single underlying sequence for saving memory.

\subsection{Condensed graph construction}
The construction process consists of two phases:
\begin{itemize}
\item original (uncondensed) \dbg{} construction step
\item condensation step
\end{itemize}

We use straightforward \dbg{} implementation over the set of $k$-mers. 

After some experiments we started using cuckoo hashset (\url{http://en.wikipedia.org/wiki/Cuckoo_hashing}) as an underlying set implementation.

During condensation step we construct condensed \dbg{} and $k$-mer index at the same time.

Condensation is done by the traversal of the \dbg{} graph edges and accumulating edge-sequence while we're on a simple path (without branches). Obtained sequences are added to the condensed graph as edges and are immediately indexed.

%The result is stored in \texttt{DeBruijnGraph} class which is much more complex structure than SeqMap, used to store initial graph. Since condensed graph has smaller size time performance of graph operation becomes less critical thus certain operations are implemented less time-efficient while more usable.

\subsubsection{Problems}
There are only minor problems in this graph construction concerning time and space efficiency which are not of much significance while applying these code to bacterial genomes. %Even if these problems become bottleneck we could always switch to A-bruijn style graph construction.

%\subsubsection{Suggestions}

\subsection{Condensed graph interface and modification}

Our condensed graph has restricted number of ways to change its contents like add vertex or split edge etc. Each of them is represented by certain public method of condensed graph class.

All modifying operation can be nominally divided into low- and high-level operations.

Low-level operations are addition/deletion of edge/vertex.

High-level operations are:
\begin{itemize}
\item Concatenation of a path.
\item Split of an edge in a given proportion. 
\item Projection of one edge to another.
\end{itemize}

Two adjacent edges can be concatenated only if their common vertex is 1-in/1-out.

Split of an edge $e$ with parameter $\alpha$ creates two new adjacent edges $e_1, e_2$ s.t. $length(e_1)/length(e_2) = \alpha$ along with their common vertex.

Projection of edge $e_1$ to $e_2$ in terms of structural changes results only in the deletion of $e_1$, but we need this operation to move all associated information from edge $e_1$ to $e_2$.

If during graph modification some vertex becomes isolated, it is deleted. Edges are concatenated immediately with each opportunity.

During graph modification it is important not to break consistency of conjugate elements. 
To achieve this every time a vertex/edge is added/deleted from the graph, conjugate element is added/deleted as well.
Since high-level operations are implemented with low-level operations stated strategy appears to be sufficient for them to work correctly out of the box in most cases. \footnote{The only special case is concatenation of the path that contains self-conjugate edge and of course edge cannot be projected to its conjugate}

%To achieve this every time a modifying operation is performed, the symmetrical operation is performed on conjugate elements. Implementation of this approach appeared to be tricky at some moments, but after it was done it allowed us to think of conjugateness much more seldom. 

What makes our condensed graph convenient and extendible is employment of callback technique. Each time a modifying operation is performed corresponding event is triggered and all objects that listen to graph events are notified about what exactly happened to the graph.% and respond to the event as they like.


 Thus instead of overloading graph structure with information and features one can create external tools which could be easily switched on and off dynamically.

\subsubsection{Problems}
\begin{itemize}
\item Code is argued to be over-architectured. In particular all GraphHandler calls are virtual and it is not yet clear if there is performance bottleneck here.
\item Graph still has different core from A-bruijn graph thus major refactoring is needed. It will be finished in the near future,
\item Initially graph was meant to be used for error correction thus operations necessary for repeat resolving are still not supported and even interface needs further discussion. Also certain problems arise concerning support of additional information which would be discussed later.
\end{itemize}

\subsection{Coverage}

Coverage of an edge here corresponds to the number of times (K+1)-mers from this edge were observed in original reads. It's additive, so when we merge edges coverage of result is equal to sum of coverages of edges merged. At the moment coverage is stored in edges of graph and maintained with Handler.

\subsubsection{Problems}

Coverage structure itself seems to have no problems but it is not clear how to support coverage values in unglueing operations used in repeat resolving. When one edge is spread among several new edges one can not tell which part of edge coverage supports which edge. Actually this problem could be just ignored because when certain ungluing operation is performed one can rely on all the new edges to be trusted thus actual value of coverage does not really matter while we do not try to estimate edge multiplicity by its coverage.

\subsubsection{Suggestions}

\subsection{Index}

In order to be able to map reads to graph index structure is maintained. Index contains map from k+1-mers to positions at edges where these k+1-mers are in graph.

\subsubsection{Problems}

It is not clear if this whole heavy structure is actually necessary. Maybe only neighbourhoods of vertices should be indexed.


\subsubsection{Suggestions}
Memory needed can be decreased to half if storing info only for lexicographically min of conjugate $k+1$-mers

\subsection{Unique ids}

Currently id of vertex or edge is an address of corresponding object in memory. As the result ids are unique at any given moment of time but if one considers all vertices or edges which were in graph while program worked ids become not unique because memory where vertex/edge object was stored at some point could be reused later and thus it would have the same id. In order to deal with this problem UniqueIdStorage structure was suggested(not yet implemented). This structure allows to assign each vertex and edge globally unique id and then find vertex with given unique id or unique if of the given vertex.

\subsection{History}

\subsection{Paired Info}

Paired info structures store information about paired reads. For each pair of edges one can request list of all information paired reads give for this pair, Also one can request information about just one pair.

%about clustering
Pair info storing itself is quite easy task while deciding on the form paired info should be stored in is much more sophisticated. Insert length has large dispersion value thus clustering of some sort is required. There are two types of clustering each of which should be implemented: online clustering and offline clustering.

There are several points concerning paired information that should be taken into consideration:

\begin{itemize}
\item The earlier reads are mapped to graph the easier this procedure is. If reads are mapped to graph before tip clipping they map to graph perfectly. If reads are mapped to graph after tip clipping but before bulge removal ends of certain reads could have been removed from graph, but mapping is still straightforward. If reads are mapped to graph after bulge removal read mapping becomes complicated task (and is actually called read threading).

\item The earlier reads are mapped to graph the larger amount of space is required to store this information. This is natural since number of edges significantly drops after graph simplification. And also edges become longer and thus lots of paired reads are have both reads from pair mapped to the same edge,

\item The later clustering is performed the more precise result is. Longer edges contain more info. Thus average values are closer to real distances.
\end{itemize}

Currently the following clustering strategy is implemented: first even before tip clipping and bulge removal data is collected from paired read and as the result for each pair of edges we have collection of possible distances paired reads vote for. Then this information is carefully clustered and result of this clusterization is the one actually used to resolve repeats, construct rectangle graph etc.

\subsubsection{Online Clustering}

If paired info is used to change graph one should synchronize paired info with these changes. Thus there should be methods to change and merge paired information.

\subsubsection{Offline Clustering}

Input data for offline clustering is collection of paired data with high variance value and output is meant to be carefully clustered information which aims to be as precise as possible.

\subsubsection{Weights}
In order to measure level of certainty paired reads give for this distance each information unit has weight value. Currently there are three ways to count weights being argued.

\begin{itemize}
\item Currently implemented is the following way: weight of pair is number of pairs of $k+1$-mers from paired reads that support this weight. For example if first read of pair intersects with sequence stored in edge by $a$ nucleotides and second read of pair intersects with sequence stored in the other edge by $b$ nucleotides than information contributed by this read pair for this pair of edges is $(a - k)(b - k)$. Advantage of this way to measure weight is that it can be easily updated since it is additive with respect to merge and glue operations.
\item First each read pair is split into sequence of pairs of k+1-mers in the following way: extract k+1-mers from both reads which start at position $i$ for all values of $i$ such that k+1-mers can be extracted. Then these pairs are mapped on graph. Weight of distance for pair of edges is the number of k+1-mer pairs that support this distance.
\item Weight of paired information is a number of reads that support this information. This weight is very difficult to support but it may be better at reflecting level of certainty in this information.
\end{itemize}

\subsubsection{Problems}

The main problem here is how to distribute paired information among new edges when unglueing operations are performed on graph. This problem can not be ignored since we can neither copy information to all new edges because we then have lots of mistakes in paired info nor can we forget about this because lots of paired info would be wasted.

\subsubsection{Suggestions}

The only solution we thought of is to make paired info support a job for repeat resolver itself. This solution breaks concept that information storages should be synchronized with by listening graph events.

\section{Graph tools}
\subsubsection{Smart Iterators}

Smart iterators are tools to iterate through vertices/edges of graph. These iterators were called smart because of two reasons:
\begin{itemize}
\item Smart Iterators do not become invalid after insertions/deletions of edges/vertices. Moreover Smart Iterators gaurantee that when iteration is finished all vertices/edges which currently belong to graph. This became possible because of Handler structure. Smart Iterators themselves are handlers and listen to add/delete edge/vertex events.
\item Smart Iterators allow to choose order in which vertices/edges are iterated. This is done with comparator that defines which vertices/edges should go first.
\end{itemize}

\section{Simplification algorithms}

\subsection{Tip Clipping}

The following algorithm is used for tip clipping: we iterate through all edges of graph in order of increasing length. If current edge topologically looks like tip and has alternative path with good enough coverage this edge is removed. If after removing this edge vertex that was adjacent with removed edge has unique incoming edge and unique outgoing edge those edges are concatenated and new edge is also checked as tip candidate.

\subsection{Bulge Removal}
\textit{Simple bulge} is the edge with low (absolute and relative) coverage such that there is an alternative path between its start and end of length $\approx$ edge length.

We iterate through all edges, looking for simple bulges. If we find such edge, we remove it, condense neighbourhoods of its ends and add new edges to traversal.

This strategy allows to iteratively delete pretty complex bulge structures.

\end{document}